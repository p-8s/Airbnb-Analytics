---
title: "ST309_Project"
author: "Paxton Quek, Patricia Yin"
date: "02/02/2021"
output: pdf_document
---

# I. Exploratory Analysis
## 1. Load Data
```{r}
#setwd("~/Documents/好好学习！/R/ST309/st309-project")
library(tidyverse)
library(car)
library(ggplot2)
library(readxl)
library(stringr)
library(rgdal)
library(dplyr)
library(data.table)
library(tree)
library(randomForest)
library(ROCR)

listings1 <- read_excel("9Dec2019_PriceAmenitiesRatings.xlsx")
listings_o <- read_excel("9Dec2019_PriceAmenitiesRatings.xlsx")
summary(listings1)
```

## 2. Price
```{r}

# Standardise all cleaning_fee such that entries with "NA" are 0 
listings1$cleaning_fee[is.na(listings1$cleaning_fee)] <- 0

# Add a new column with normalised prices (pro-rata per person for a 3-day stay)
# this is to account for the cleaning_fee, which can be substantial
listings1$price_n <- with(listings1,(price*3 + cleaning_fee)/3/guests_included)

summary(listings1$price_n)
#the skewness in price_n is too extreme, apply log(price_n + 1) transformaiton
listings1$price_n <- log(listings1$price_n + 1)
#rename column
listings1 <- listings1 %>% rename(log_price_n =price_n)
```

## 3. host_since
```{r}
#host_since 
host_since_n <- listings1$host_since

#change host_since to years of operating, round up/down to nearest year
host_since_n <- as.numeric(round((as.Date("2020-02-08", format = "%Y-%m-%d") - as.Date(host_since_n, format = "%Y-%m-%d"))/365))

listings1$host_since_n <- host_since_n
```

## 4. host_response_time
```{r}
host_response_time_n <- as.factor(listings1$host_response_time)
#combine NA and N/A as N/A because this level might be significant in further analysis.
host_response_time_n[is.na(host_response_time_n)] <- "N/A"
listings1$host_response_time_n <- host_response_time_n
```

## 5. host_response_rate
```{r}
host_response_rate_n <- as.numeric(listings1$host_response_rate)
hist(host_response_rate_n)
summary(host_response_rate_n)
#recode the variable as factor: 100% is one level, not 100% another, NA the third
host_response_rate_n[host_response_rate_n != 1] <- "Not100"
host_response_rate_n[host_response_rate_n == 1] <- "100"
host_response_rate_n[is.na(host_response_rate_n)] <- "N/A"
host_response_rate_n <- as.factor(host_response_rate_n)
listings1$host_response_rate_n <- host_response_rate_n
```

## 6. host_is_superhost
```{r}
summary(as.factor(listings1$host_is_superhost))
superhost_n <- as.factor(listings1$host_is_superhost)
levels(superhost_n) <- c(F, T)
listings1$superhost_n <- superhost_n
#note that host_response_time has 944 NA, host_response_rate has 944 NA, host_is_superhost has 944 NA, investigation below
```

## 7. 944 entries w zero data for host

```{r}
#investigate:
compare_1 <- listings_o$id[is.na(listings_o$host_response_time)] == listings_o$id[is.na(listings_o$host_response_rate)]
length(compare_1[compare_1 == FALSE])
compare_2 <- listings_o$id[is.na(listings_o$host_response_rate)] == listings_o$id[is.na(listings_o$host_is_superhost)]
length(compare_2[compare_2 == FALSE])
#it is the same listings which has NA for all three columns
host_NA.dat <- subset(listings_o, is.na(host_response_rate))
head(host_NA.dat)
sample(host_NA.dat$id, size =3)
#remove the 944 entries
listings1 <- listings1[!is.na(listings1$host_since),]

```

By randomly checking five of the listings, the information about host is actually available on Airbnb. We believe the NA in the dataset might be due to some technical issues. Removing them from the dataset will not introduce significant bias. 

## 8. host verification

```{r}
host_vn <- listings1$host_verifications

#convert empty entry in host_verification to NA & save NA rows numbers
NArows <- c()
for (i in 1:length(host_vn)) {
  if (host_vn[i] == "[]") {
    host_vn[i] = NA
    NArows <- c(NArows, i)
  }
}

#split word entries in the host_verification
host_vnList <- strsplit(host_vn, split =",")

#clean host_vn by removing the signs 
remove.sign <- function (df) {
  str_replace_all(df, "[^[:alnum:]]", "")
  }
host_vnClean <- lapply(host_vnList, remove.sign)

#code the name of each list in the list of lists using their rownumber
names(host_vnClean) <- 1:length(host_vnClean)
#this is a precaution to deal with messed up row numbers after removing NAs

#get a list of all possible methods
getAllMethods <- function (LOL) {
  methods <- c()
  checks <- c()
  for (i in 1:length(LOL)) {
    for (j in 1:length(LOL[[i]])) {
      if (!(LOL[[i]][j] %in% methods)) {
        methods <- c(methods, LOL[[i]][j])
      }
    }
  }
  return(methods)
}

#NA is removed from the verification methods list
methods <- getAllMethods(host_vnClean)[1:20]


#take each method as a dimension, recode host_vnClean to a list of vectors
host_vnNew = matrix(0, ncol = length(methods),nrow = length(host_vnClean))
colnames(host_vnNew) <- methods

for (i in 1:length(host_vnClean)) {
  for (m in 1:length(methods)) {
    if (methods[m] %in% host_vnClean[[i]]) {
      host_vnNew[i,m] <- 1
    }
  }
}

#perform kmeans clustering
set.seed(130)
km.out <- kmeans(host_vnNew, 4, nstart = 100)
kmeans_results <- km.out$cluster
kmeans_centroids <- km.out$centers
for (i in 1:4) {print(length(which(kmeans_results == i)))}

listings1$host_verification_n <- kmeans_results
```

## 9. street

```{r}
street_n <- as.factor(listings1$street)
summary(street_n)
#many do not have an exact street name, not considered further.
```

## 10. neighbourhood_cleansed

```{r}
neig_n <- as.factor(listings1$neighbourhood_cleansed)
neig_dat <- as.data.frame(summary(neig_n))
neig_dat <- rownames_to_column(neig_dat)
colnames(neig_dat) <- c("neighbourhood", "listings")

#get the frequency of superhosts in each neighbourhood
neig_superhost <- as.data.frame(cbind(listings1$neighbourhood_cleansed, listings1$host_is_superhost))
colnames(neig_superhost) <- c("neighbourhood", "superhost") 
neig_superhost[,c(1:2)] <- lapply(neig_superhost[,c(1:2)], as.factor)

neig_sT <- neig_superhost$neighbourhood[neig_superhost$superhost == "t"]
neig_T_dat <- as.data.frame(summary(neig_sT))
neig_T_dat <- rownames_to_column(neig_T_dat)
colnames(neig_T_dat) <- c("neighbourhood", "superhosts")

#merge datasets for no. number of listings & no. of superhosts in each neighbourhood
neig_l_s <- merge(neig_dat, neig_T_dat, by = "neighbourhood")
neig_l_s$perc_superhost <- neig_l_s$superhosts / neig_l_s$listings

#rank boroughs according to no. of listings
n_sort_listings <- neig_l_s[order(neig_l_s$listings, decreasing = T),]
#rank boroughs according to percentage of superhost
n_sort_percsup <- neig_l_s[order(neig_l_s$perc_superhost, decreasing = T),]


```

## 11. latitude and longitude

```{r}
lis_location <- listings1[c("zipcode", "latitude", "longitude", "host_is_superhost")]
lis_location$host_is_superhost <- as.factor(lis_location$host_is_superhost)
summary(lis_location)

#load the shapefile of london boroughs
ldn_boroughs <- readOGR(dsn = "London-boroughs/London_Borough_Excluding_MHW.shp") #the whole london-boroughs folder must be downloaded because .shp (shapefile) is composed by different files in the folder.
#transform the coordinates to longitude and latitude
proj4string(ldn_boroughs) <- CRS("+init=epsg:27700")
ldn_boroughs.wgs84 <- spTransform(ldn_boroughs, CRS("+init=epsg:4326"))
#add a column of id for matching with dataframe containing borough name
ldn_boroughs.wgs84$id <- row.names(ldn_boroughs.wgs84)

##(11.1) map one: visualise percentage of superhosts on map of london boroughs
#labels for polygons (boroughs), positioned at the centre of each polygon
centroid.dat <- as.data.frame(coordinates(ldn_boroughs.wgs84))
centroid.dat <- rownames_to_column(centroid.dat, "id")
colnames(centroid.dat) <- c("id", "long", "lat")
polygon_label.dat <- left_join(centroid.dat, ldn_boroughs.wgs84@data, by="id")

#merge shapefile data with borough name data and count of listings / superhosts data
ldn_boroughs.wgs84@data <- left_join(ldn_boroughs.wgs84@data, neig_l_s, by = c("NAME" = "neighbourhood"))
ldn_boroughs_f <- fortify(ldn_boroughs.wgs84)
ldn_boroughs_m <- left_join(ldn_boroughs_f, ldn_boroughs.wgs84@data, by = "id")

map01 <- ggplot() + geom_polygon(data=ldn_boroughs_m, mapping=aes(long, lat, group=`NAME`, fill=`perc_superhost`)) + geom_path(color="white", lwd=0.1) + coord_equal() + scale_fill_gradient(high="#132B43", low="#56B1F7", name = "% of superhosts")
map02 <- map01 + geom_text(data=polygon_label.dat, mapping=aes(x=long, y=lat, label=NAME), size=3) + ggtitle("Percentage of superhosts in each borough")

##(11.2) map two: visualise listings on map of london boroughs
#plot map with listings data
map1 <- ggplot() + geom_polygon(data=ldn_boroughs_m, mapping=aes(long, lat, group = group, fill=NAME)) + geom_path(data=ldn_boroughs_m, mapping=aes(long, lat, group=group), color="white", lwd=0.1)
map2 <- map1 + geom_point(data=lis_location, mapping=aes(longitude, latitude), color="dimgrey", size=0.05) + coord_equal() + ggtitle("Airbnb listings in London")
map3 <- map2 + theme(legend.position = "none") #map without legend (names of borough)

#transform location data to distance to central london
cent_ldn_lat <- 51.509865
cent_ldn_long <- -0.118092

mil_to_centre_n <- sqrt(((lis_location$longitude - cent_ldn_long)*54.6)^2 + ((lis_location$latitude - cent_ldn_lat)*69)^2)
summary(mil_to_centre_n)

listings1$mil_to_centre_n <- mil_to_centre_n
```

[1]  https://toboroughsdatascience.com/plotting-a-map-of-london-crime-data-using-r-8dcefef1c397 the reference for transformation of coordinates to longitude and latitude
[2]  https://www.latlong.net/place/london-the-uk-14153.html centre of london longitude and latitude
[3] https://github.com/valeria-io/crime_london_map (11.2) map 1

- code for maps with uniform colours

map0 <- ggplot() + geom_polygon(data=ldn_boroughs.wgs84, mapping=aes(long, lat, group = group), fill="skyblue2") + geom_path(data=ldn_boroughs.wgs84, mapping=aes(long, lat, group = group), color="white", lwd=0.1)
map0.1 <- map0 + geom_point(data=lis_location, mapping=aes(longitude, latitude), color = "dimgrey", size=0.05) + ggtitle("Airbnb listings in London") + theme(legend.position = "none")


## 12. is_location_exact
```{r}
summary(as.factor(listings1$is_location_exact))
```

## 13. property type
```{r}
#check price difference among different types (original)
property_n2 <- listings1$property_type
prop_o_price <- as.data.frame(cbind(property_n2, listings1$price))
prop_o_price$price_n <- as.numeric(prop_o_price$V2)
ggplot(data=prop_o_price, aes(x=factor(property_n2), y=log(price_n)))+geom_boxplot()

#group levels according to common understanding of room types
property_n <- listings1$property_type
summary(as.factor(property_n))
property_n[property_n == "Aparthotel" |  property_n == "Loft" | property_n == "Serviced apartment"] <- "Apartment"
property_n[property_n == "Bungalow" |property_n == "Cottage" | property_n == "Tiny house" | property_n == "Townhouse" ] <- "House"
property_n[property_n == "Villa"] <- "Condominium"
property_n[property_n != "Apartment" & property_n != "House" & property_n != "Condominium"] <- "Others"

#check price difference among different types
prop_price <- as.data.frame(cbind(property_n, listings1$price))
prop_price$price <- as.numeric(prop_price$V2)
ggplot(data=prop_price, aes(x=factor(property_n), y=log(price)))+geom_boxplot()
mean(prop_price$price[prop_price$property_n == "Apartment"])

listings1$property_type_n <- as.factor(property_n)
```

## 14. room types

```{r}
summary(as.factor(listings1$room_type))
listings1$room_type <- as.factor(listings1$room_type)
```


## 15. bed types

```{r}
summary(as.factor(listings1$bed_type))
#change to real bed and non real bed as other categories have too few data
bed_type_n <- listings1$bed_type
bed_type_n[bed_type_n != "Real Bed"] <- "Non Bed"
summary(as.factor(bed_type_n))
listings1$bed_type_n <- as.factor(bed_type_n)
```

## 16. amenities

```{r}
amenities_n <- listings1$amenities
head(amenities_n)
amenities_list <- strsplit(amenities_n, ",")
head(amenities_list)

#clean amenities list
remove.punc <- function(mylist) {str_remove_all(mylist, "[:punct:]")}
a_clean_list <- lapply(amenities_list, remove.punc)


#plot histogram of no. of amenities for listings
total_amenities <- as.numeric(summary(a_clean_list)[,1])
hist(total_amenities, 
     main = "Histogram of no. of amenities for listings",
     xlab = "Number of amenities",
     ylab = "Number of listings")

#check relationship between no. of total amenities and host_is_superhost
total_sup <- as.data.frame(cbind(total_amenities, listings1$host_is_superhost))
total_sup$total_amenities <- as.numeric(total_sup$total_amenities)
total_sup$V2 <- as.factor(total_sup$V2)
boxplot(total_sup$total_amenities ~ total_sup$V2)

#find all possible items provided
raw_items <- getAllMethods(a_clean_list)

#find the frequency that each item appears
getMethodFreq <- function (LOL, item_list){
  freq_list <- matrix(0, nrow = length(item_list), ncol = 1)
  rownames(freq_list) <- item_list
  colnames(freq_list) <- "freq"
  
  for (m in 1:length(item_list)) {
    count = 0
    for (i in 1:length(LOL)) {
      if (item_list[m] %in% LOL[[i]]) {
        count = count + 1
      }
    freq_list[m][1] = count
    }
  }
  return (freq_list)
}

#get the frequency that each item appears
item_freq <- getMethodFreq(a_clean_list, raw_items)
item_freq <- as.data.frame(item_freq)
item_freq <- rownames_to_column(item_freq, "item")
item_freq <- item_freq[order(item_freq$freq, decreasing = T),]


#count the number of items provided by listings that are among the top N amenities
countTopN <- function (LOL, topN) {
  count_matrix = matrix(NA, nrow = length(LOL), ncol = 1)
    
  for (i in 1:length(LOL)) {
    count = 0
    for (m in 1:length(topN)) {
      if (topN[m] %in% LOL[[i]]) {count = count + 1}
    }
    count_matrix[i][1] = count
  }
  
  return(count_matrix)
}

#(1)check top 20 amenities: count, distribution, relationship to host_is_superhost
top20amen <- item_freq$item[1:20]
top20_count_matrix <- countTopN(a_clean_list, top20amen)
head(top20_count_matrix)

##(1.2)distribution
hist(top20_count_matrix[,1])

##(1.3)rship between no. of items in top20 amenities and host_is_superhost
top20_sup <- as.data.frame(cbind(top20_count_matrix, listings1$host_is_superhost))
colnames(top20_sup) <- c("top20_count", "is_superhost")
top20_sup$top20_count <- as.numeric(top20_sup$top20_count)
top20_sup$is_superhost <- as.factor(top20_sup$is_superhost)
boxplot(top20_sup$top20_count ~ top20_sup$is_superhost) 
t.test(top20_count ~ is_superhost, data = top20_sup) #difference between two levels: significant

#(2)check top 10 amenities:
top10amen <- item_freq$item[1:10]
top10_count_matrix <- countTopN(a_clean_list, top10amen)
head(top10_count_matrix)

##(2.2)distribution
hist(top10_count_matrix[,1])

##(2.3)rship between no. of items in top10 amenities and host_is_superhost
top10_sup <- as.data.frame(cbind(top10_count_matrix, listings1$host_is_superhost))
colnames(top10_sup) <- c("top10_count", "is_superhost")
top10_sup$top10_count <- as.numeric(top10_sup$top10_count)
top10_sup$is_superhost <- as.factor(top10_sup$is_superhost)
boxplot(top10_sup$top10_count ~ top10_sup$is_superhost) #visually: not highly significant
t.test(top10_count ~ is_superhost, data = top10_sup)

#(3)check top 30 amenities:
top30amen <- item_freq$item[1:30]
top30_count_matrix <- countTopN(a_clean_list, top30amen)
head(top30_count_matrix)

##(3.2)distribution
hist(top30_count_matrix[,1])

##(3.3)rship between no. of items in top30 amenities and host_is_superhost
top30_sup <- as.data.frame(cbind(top30_count_matrix, listings1$host_is_superhost))
colnames(top30_sup) <- c("top30_count", "is_superhost")
top30_sup$top30_count <- as.numeric(top30_sup$top30_count)
top30_sup$is_superhost <- as.factor(top30_sup$is_superhost)
boxplot(top30_sup$top30_count ~ top30_sup$is_superhost) #visually: not highly significant
t.test(top30_count ~ is_superhost, data = top30_sup)

#add new column to listings1
listings1$amen_count <- total_amenities
```

conclusion 1: the ten most frequently appeared amenities for listings on Airbnb:
 [1] "Wifi"           "Essentials"     "Heating"        "Kitchen"       
 [5] "Smoke detector" "Washer"         "Hangers"        "Iron"          
 [9] "Hair dryer"     "Shampoo"      

conclusion 2: superhosts tend to provide more amenities than non-superhosts

## 17. cancellation_policy

```{r}
cancel_n <- as.factor(listings1$cancellation_policy)
summary(cancel_n)
cancel_n[cancel_n != "flexible" & cancel_n != "moderate"] <- "strict"
cancel_n <- droplevels(cancel_n)
listings1$cancellation_policy_n <- cancel_n
```

Note: within level "strict", 23574 out of 23897 are "strict_14_with_grace_period". The others levels were merged into strict because they have very small presence.

## 18. guest verification

```{r}

summary(listings1$require_guest_profile_picture) #628 t
summary(as.factor(listings1$require_guest_phone_verification)) #999 t

#check rlship between phone_verifcation and profile-pic-verif
length(which(listings1$require_guest_profile_picture == "t" & listings1$require_guest_phone_verification == "t")) #557 t

#transform verification
guest_verif <- as.data.frame(cbind(listings1$require_guest_profile_picture, listings1$require_guest_phone_verification, 0))
colnames(guest_verif) <- c("profil_pic", "phone", "any_form")
for (i in 1:nrow(guest_verif)) {
  if (guest_verif$profil_pic[i] == "t" | guest_verif$phone[i] == "t") {
    guest_verif$any_form[i] <- T
  } else {guest_verif$any_form[i] <- F}
}
summary(as.factor(guest_verif$any_form))

listings1$guest_verif_n <- as.factor(guest_verif$any_form)
```

557 listings require both phone and profile pic verifications
628-557 require only phone
999-557 require only profile pic

## 19. response variable
```{r}
#check no. of reviews
summary(listings1$reviews_per_month) #median= 0.9
hist(listings1$reviews_per_month)
length(which(listings1$reviews_per_month > 0.9)) #25602

#check review score
hist(listings1$review_scores_rating, breaks = 50)
summary(listings1$review_scores_rating) #median = 95
length(which(listings1$review_scores_rating > 95)) #25298

popular.dat <- as.data.frame(cbind(listings1$reviews_per_month, listings1$review_scores_rating))
popT.index <- which(popular.dat$V1 > 0.9 & popular.dat$V2 > 95)
popular.dat$is_popular[popT.index] <- T
popular.dat$is_popular[-popT.index] <- F

listings1$is_popular <- popular.dat$is_popular
```


# II. Data Analysis

## 1. Preparing data

```{r}
#subset to include only the relevant ones
colnames(listings1)
relevant_col = c("log_price_n", "host_since_n", "host_response_time_n", "host_response_rate_n", "host_verification_n", "mil_to_centre_n", "property_type_n", "bed_type_n", "amen_count", "cancellation_policy_n", "guest_verif_n", "room_type", "superhost_n", "accommodates","minimum_nights", "reviews_per_month", "review_scores_rating", "is_popular")
listings2 <- listings1[,relevant_col]
summary(listings2)
#remove two rows with NA in reviews_per_month
listings2 <- listings2[complete.cases(listings2),]
```

Response variable: host_is_superhost

Remove apparently non-relevant columns: id, host_id, host_name, 

Remove not suitable ones: 
1. host_about: (more detailed text analysis in the later section)
2. host_listings_count / host_total_listings_count : we are interested in how to price a listing and (how to improve a listing in order to become a superhost). this variable is not directly relevant.  
3. "bathrooms", "bedrooms", "beds", guests_included, extra_people: they are correlates to how many people accommodated
4. calculated_host_listings_count
calculated_host_listings_count_entire_homes
calculated_host_listings_count_private_rooms
calculated_host_listings_count_shared_rooms: (not relevant?)

Select one variable to represent similar variables:
1. {host verification_n: host_verification, host_has_profile_pic, host_identity_verified}
2. location {mil_to_centre_n: host_location, street, neighbourhood_cleansed, city, zipcode, smart_location, latitude, longitude, is_location_exact}
4. price {price_n: price, weekly_price, security_deposit, cleaning_fee, }
5. no of reviews {reviews_per_month: number_of_reviews_ltm, number_of_reviews}
6. reviews rating {review_scores_rating: review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value}
6. guest verif {guest_verif_n: require_guest_profile_picture,  require_guest_phone_verification}

Transformation needed for any variables?

- Some variables appear to have heavy tails (or outliers) in comparing with normal distributions, as they minimum and maximum values are way out of the ranges of mean ±2×SDT. [copied from solution 6]. As applying normalisation or standardisation may make interpretation harder and disrupt the natural dependency structure within data, we prefer to leave variables to its original form.


## 2. Model1 - glm popular

```{r}
#create a confusion matrix
ct.op <- function(predicted, observed) {
  df.op <- data.frame(predicted = predicted, observed = observed)
  op.tab <- table(df.op)
  op.tab <- rbind(op.tab, c(round(prop.table(op.tab, 2)[1, 1], 2), round((prop.table(op.tab, 2)[2, 2]), 2)))
  rownames(op.tab) <- c("pred=F", "pred=T", "%corr")
  colnames(op.tab) <- c("obs=F", "obs=T")
  op.tab
}

#remove the two variables that were used to construct is_popular
glm1 <- glm(is_popular ~. - reviews_per_month - review_scores_rating, family=binomial, data=listings2)
summary(glm1)
#set threshold as 0.5
pred.m1 <- as.numeric(glm1$fitted.values > 0.5) 
ct.op(pred.m1, listings2$is_popular) #0.92, 0.45

#remove non-significant variables one by one
#remove host_verification_n
glm2 <- glm(is_popular ~. - reviews_per_month - review_scores_rating - host_verification_n, family=binomial, data=listings2)
anova(glm2, test="Chisq") #test the overall significance of a variable
#set threshold as 0.5
pred.m2 <- as.numeric(glm2$fitted.values > 0.5) 
ct.op(pred.m2, listings2$is_popular) #0.92, 0.45

#remove bed_type_n
glm3 <- glm(is_popular ~. - reviews_per_month - review_scores_rating - host_verification_n - bed_type_n, family=binomial, data=listings2)
anova(glm3, test="Chisq")
#set threshold as 0.5
pred.m3 <- as.numeric(glm3$fitted.values > 0.5) 
ct.op(pred.m3, listings2$is_popular) #0.92, 0.45

#remove minimum_nights
glm4 <- glm(is_popular ~. - reviews_per_month - review_scores_rating - host_verification_n - bed_type_n - minimum_nights, family=binomial, data=listings2)
anova(glm4, test="Chisq")
#set threshold as 0.5
pred.m4 <- as.numeric(glm4$fitted.values > 0.5) 
ct.op(pred.m4, listings2$is_popular) #0.92, 0.45

#remove log_price_n
glm5 <- glm(is_popular ~. - reviews_per_month - review_scores_rating - host_verification_n - bed_type_n - minimum_nights - log_price_n, family=binomial, data=listings2)
anova(glm5, test="Chisq")
#set threshold as 0.5
pred.m5 <- as.numeric(glm5$fitted.values > 0.5) 
ct.op(pred.m5, listings2$is_popular) #0.92, 0.45

#try interaction between most significant ones
glm6 <- glm(is_popular ~ host_since_n + host_response_time_n + host_response_rate_n + property_type_n + amen_count + cancellation_policy_n + guest_verif_n + room_type + mil_to_centre_n * superhost_n + accommodates, family=binomial, data=listings2)
anova(glm6, test="Chisq")
#set threshold as 0.5
pred.m6 <- as.numeric(glm6$fitted.values > 0.5) 
ct.op(pred.m6, listings2$is_popular) #0.92, 0.45
#significant but does not improve on confusion matrix, disregard glm6

#set threshold
set.seed(101)
g.train.index <- sample(1:dim(listings2)[1], dim(listings2)[1] * 0.8) #80:20 split
g.train.dat <- listings2[g.train.index,]
g.test.dat <- listings2[-g.train.index,]

glm5tr <- glm(is_popular ~ host_since_n + host_response_time_n + host_response_rate_n + property_type_n + amen_count + cancellation_policy_n + guest_verif_n + room_type + mil_to_centre_n + superhost_n + accommodates, data=g.train.dat)
pred.train <- glm5tr$fitted.values

glm5te <- glm(is_popular ~ host_since_n + host_response_time_n + host_response_rate_n + property_type_n + amen_count + cancellation_policy_n + guest_verif_n + room_type + mil_to_centre_n + superhost_n + accommodates, data=g.test.dat)
pred.test <- glm5te$fitted.values

prediction.test <- prediction(pred.test, g.test.dat$is_popular)
prediction.train <- prediction(pred.train, g.train.dat$is_popular)

#
roc.test <- performance(prediction.test, measure="tpr", x.measure="fpr")
roc.train <- performance(prediction.train, measure="tpr", x.measure="fpr")
par(mfrow=c(1,2))
plot(roc.test, lwd=2, colorkey=T, colorize=T, main="ROC curve on testing data")
abline(0,1)
plot(roc.train, lwd=2, colorkey=T, colorize=T, main="ROC curve on training data")
abline(0,1)

#try glm5, still
pred.m5t <- as.numeric(glm5$fitted.values > 0.1) 
ct.op(pred.m5t, listings2$is_popular) #0.92, 0.45

#create a more balanced dataset by having equal numbers of T and F for is_popular
getBalanceData <- function (dfT, dfF) {
  Fsam_index <- sample(dim(dfF)[1], size = dim(dfT)[1])
  df_Fsam <- dfF[Fsam_index,]
  df_bal <- rbind(dfT, df_Fsam)
  return(df_bal)
}

listings_T <- listings2[listings2$is_popular == T,]
listings_F <- listings2[listings2$is_popular == F,]
set.seed(1)
listings_bal1 <- getBalanceData(listings_T, listings_F)

glm_bal1 <- glm(is_popular ~. - reviews_per_month - review_scores_rating, family = binomial, data = listings_bal1)
anova(glm_bal1, test="Chisq")
pred.m <- as.numeric(glm_bal1$fitted.values > 0.5) 
ct.op(pred.m, listings_bal1$is_popular) #0.81, 0.64

```

## 3. Model2 - decision tree - popular

```{r}
#remove two variables used to construct is_popular
listings_dt <- subset(listings2, select=-c(reviews_per_month, review_scores_rating))

set.seed(12)
tree.pop1 <- tree(as.factor(is_popular) ~., data=listings_dt)
summary(tree.pop1) #mis = 0.189
plot(tree.pop1)
text(tree.pop1, pretty=0, cex=0.6)

#assess perfomance - 80:20 split for training and testing
nrow(listings_dt) * 0.8
set.seed(14)
train.index=sample(1:nrow(listings_dt), 41188) #80% of original data used for training
train.dat <- listings_dt[train.index,]
test.dat <- listings_dt[-train.index,]
popular_test <- test.dat$is_popular
tree.pop2 <- tree(as.factor(is_popular) ~., data=train.dat)
summary(tree.pop2) #mis = 0.1882
plot(tree.pop2) 
text(tree.pop2, pretty=0, cex=0.6)
tree.pop2.predict <- predict(tree.pop2, test.dat, type="class")
tree.test.conf <- table(tree.pop2.predict, popular_test) 
(tree.test.conf[2,1] + tree.test.conf[1,2]) / nrow(test.dat) #mis = 0.193
#poor performance on predicting T

#improve decision tree: bagging
dim(listings_dt)
#! Caution: bagging takes more than 7 min to run, check below for results
# bag.pop <- randomForest(as.factor(is_popular)~., data=listings_dt, subset=train.index, mtry=15, importance=T) #mtry=15 uses all predictor in the dataset
# importance(bag.pop)

#improve decision tree: randomForest
#! Caution: it takes a bit of time to run
# rf.pop <- randomForest(as.factor(is_popular)~., data=listings_dt, subset=train.index, mtry=4, importance=T) #mtry sets m=4 close to sqrt(p=15)
# importance(rf.pop)

#cross validation to determine the tree size
cv.pop <- cv.tree(tree.pop1, FUN=prune.misclass)
cv.pop$size
cv.pop$dev
#pop1 is indeed the cv selected tree
```

! results for bagging: 

Call:
 randomForest(formula = as.factor(is_popular) ~ ., data = listings_dt,      mtry = 15, importance = T, subset = train.index) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 15

        OOB estimate of  error rate: 18.65%
Confusion matrix:
      FALSE TRUE class.error
FALSE 29417 2479  0.07772134
TRUE   5203 4089  0.55994404

! results for random forest

Call:
 randomForest(formula = as.factor(is_popular) ~ ., data = listings_dt,      mtry = 4, importance = T, subset = train.index) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 4

        OOB estimate of  error rate: 18.1%
Confusion matrix:
      FALSE TRUE class.error
FALSE 29637 2259  0.07082393
TRUE   5197 4095  0.55929832

- bagging and rf suggests the decision tree cant be further improved

```{r}
#use a more balanced dataset to fit deicion tree
listings_dt_T <- listings_dt[which(listings_dt$is_popular == T),]
listings_dt_F <- listings_dt[which(listings_dt$is_popular == F),]
set.seed(16)
listings_dt_bal1 <- getBalanceData(listings_dt_T, listings_dt_F)

tree.bal1 <- tree(is_popular ~., data=listings_dt_bal1)
summary(tree.bal1) #mis = 0.189
plot(tree.bal1)
text(tree.bal1, pretty=0, cex=0.6)

set.seed(18)
train.bal1.index=sample(1:nrow(listings_dt_bal1), nrow(listings_dt_bal1) * 0.8) #80% of original data used for train.bal1
train.bal1.dat <- listings_dt_bal1[train.bal1.index,]
test.bal1.dat <- listings_dt_bal1[-train.bal1.index,]
popular_test.bal1 <- test.bal1.dat$is_popular
tree.bal2 <- tree(as.factor(is_popular) ~., data=train.bal1.dat)
summary(tree.bal2) #mis = 0.2964
plot(tree.bal2) 
text(tree.bal2, pretty=0, cex=0.6)
tree.bal2.predict <- predict(tree.bal2, test.bal1.dat, type="class")
tree.test.bal1.conf <- table(tree.bal2.predict, popular_test.bal1) 
(tree.test.bal1.conf[2,1] + tree.test.bal1.conf[1,2]) / nrow(test.bal1.dat) #mis = 0.285

#the balanced dataset does not bring a better decision tree.
```


## 4. Model3 - mlr
```{r}
lm.p <- lm(log_price_n ~ ., data=listings2)
summary(lm.p)

lm.p <- lm(log_price_n ~ mil_to_centre_n + room_type, data=listings2)
summary(lm.p)
```