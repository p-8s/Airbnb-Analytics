---
title: "ST309_Project"
author: "Paxton Quek, Patricia Yin"
date: "02/02/2021"
output: pdf_document
---

## 2. Data Cleansing
In this section, we load the data and transform certain variables prior to analysis. Certain records are also removed to reduce the size of our dataset. 
```{r}
# Load packages
library(tidyverse)
library(car)
library(ggplot2)
library(readxl)
library(stringr)
library(rgdal)
library(dplyr)
library(data.table)
library(tree)
library(randomForest)
library(ROCR)
library(tidytext)
library(wordcloud2)

# Load data
listings1 <- read_excel("listings1.xlsx")
listings_o <- read_excel("listings1.xlsx")

```


## 2.3 Removing Rows (Listings)
There are records where host-related data is missing, but we managed to find these hosts via their id's. We believe this to be due to an error during the scraping process, so we will remove the records with incomplete information. To further reduce the amount of records 
for computational reasons, we restricted ourselves to listings that allow short-term
stays as well. We are left with 39178 observations out of the original 86469.
```{r}

# Remove records where important variables are missing
listings1 <- read_excel("listings1.xlsx")

varlist = c('host_since','host_response_rate','host_response_time',
              'host_is_superhost','host_has_profile_pic',
              'review_scores_rating','review_scores_accuracy',
              'review_scores_cleanliness','review_scores_checkin',
              'review_scores_communication','review_scores_location',
              'review_scores_value','reviews_per_month')

varn = length(varlist)
for (i in 1:varn){
  variablelabel = varlist[i]
  listings1 <- listings1[!is.na(listings1[,variablelabel]),]
}

# There exists "N/A" in certain variables - rows can be removed
listings1 <- listings1[!(listings1[,"host_response_rate"]=="N/A"),]
listings1 <- listings1[!(listings1[,"host_response_time"]=="N/A"),]

# Remove records where minimum stay exceeds 3 days
listings1 <- listings1[listings1$minimum_nights<=3,]

```


## 2.3 Initial Transformations: Price
We transformed and normalised our listing prices as the expected price of a 3 day stay per person, because cleaning fees can significantly increase the tru price of listings. 
```{r}
# Price transformation
# Standardise all cleaning_fee such that entries with "NA" are 0 
listings1$cleaning_fee[is.na(listings1$cleaning_fee)] <- 0
# Add a new column with normalised prices (pro-rata per person for a 3-day stay)
# this is to account for the cleaning_fee, which can be substantial
listings1$price_n <- with(listings1,(price*3 + cleaning_fee)/3/guests_included)
summary(listings1$price_n)
#the skewness in price_n is too extreme, apply log(price_n + 1) transformaiton
listings1$price_n <- log(listings1$price_n + 1)
#rename column
listings1 <- listings1 %>% rename(log_price_n =price_n)
```


## 3 Data Exploration
## 3.1 Host-related Data

#### host_since
To obtain the number of years the host has operated - host_since_n is calculated by subtracting host_since from the current date.
```{r}
#host_since 
host_since_n <- listings1$host_since

#change host_since to years of operating, round up/down to nearest year
host_since_n <- as.numeric(round((as.Date("2020-02-08", format = "%Y-%m-%d") - as.Date(host_since_n, format = "%Y-%m-%d"))/365))

listings1$host_since_n <- host_since_n
```

#### host_response_time
A factor representing the different response levels of hosts. 
```{r}
host_response_time_n <- as.factor(listings1$host_response_time)
listings1$host_response_time_n <- host_response_time_n
```

#### host_response_rate
A factor dummy variable representing if hosts have a 100% response rate.
```{r}
host_response_rate <- as.numeric(listings1$host_response_rate)
hist(host_response_rate)
summary(host_response_rate)
#recode the variable as factor: 100% is one level, not 100% another
host_response_rate_n[host_response_rate != 1] <- "Not100"
host_response_rate_n[host_response_rate == 1] <- "100"
host_response_rate_n <- as.factor(host_response_rate_n)
listings1$host_response_rate_n <- host_response_rate_n
```

#### host_is_superhost
A similar factor dummy variable to state if hosts are Superhosts.
```{r}
summary(as.factor(listings1$host_is_superhost))
superhost_n <- as.factor(listings1$host_is_superhost)
levels(superhost_n) <- c(F, T)
listings1$superhost_n <- superhost_n
```


#### host_verifications
To process the types of verifications used by hosts, we had to significantly recode the original character-type data. We proceeded to run a clustering analysis to check for patterns, if any, in verification preferences. 
```{r}
host_vn <- listings1$host_verifications

#convert empty entry in host_verification to NA & save NA rows numbers
NArows <- c()
for (i in 1:length(host_vn)) {
  if (host_vn[i] == "[]") {
    host_vn[i] = NA
    NArows <- c(NArows, i)
  }
}

#split word entries in the host_verification
host_vnList <- strsplit(host_vn, split =",")

#clean host_vn by removing the signs 
remove.sign <- function (df) {
  str_replace_all(df, "[^[:alnum:]]", "")
  }
host_vnClean <- lapply(host_vnList, remove.sign)

#code the name of each list in the list of lists using their rownumber
names(host_vnClean) <- 1:length(host_vnClean)
#this is a precaution to deal with messed up row numbers after removing NAs

#get a list of all possible methods
getAllMethods <- function (LOL) {
  methods <- c()
  checks <- c()
  for (i in 1:length(LOL)) {
    for (j in 1:length(LOL[[i]])) {
      if (!(LOL[[i]][j] %in% methods)) {
        methods <- c(methods, LOL[[i]][j])
      }
    }
  }
  return(methods)
}

#NA is removed from the verification methods list
methods <- getAllMethods(host_vnClean)[1:20]


#take each method as a dimension, recode host_vnClean to a list of vectors
host_vnNew = matrix(0, ncol = length(methods),nrow = length(host_vnClean))
colnames(host_vnNew) <- methods

for (i in 1:length(host_vnClean)) {
  for (m in 1:length(methods)) {
    if (methods[m] %in% host_vnClean[[i]]) {
      host_vnNew[i,m] <- 1
    }
  }
}

#perform kmeans clustering
set.seed(130)
km.out <- kmeans(host_vnNew, 4, nstart = 100)
kmeans_results <- km.out$cluster
kmeans_centroids <- km.out$centers
for (i in 1:4) {print(length(which(kmeans_results == i)))}

listings1$host_verification_n <- kmeans_results
```

## 3.2 Location Data
## neighbourhood_cleansed
```{r}
neig_n <- as.factor(listings1$neighbourhood_cleansed)
neig_dat <- as.data.frame(summary(neig_n))
neig_dat <- rownames_to_column(neig_dat)
colnames(neig_dat) <- c("neighbourhood", "listings")

#get the frequency of superhosts in each neighbourhood
neig_superhost <- as.data.frame(cbind(listings1$neighbourhood_cleansed, listings1$host_is_superhost))
colnames(neig_superhost) <- c("neighbourhood", "superhost") 
neig_superhost[,c(1:2)] <- lapply(neig_superhost[,c(1:2)], as.factor)

neig_sT <- neig_superhost$neighbourhood[neig_superhost$superhost == "t"]
neig_T_dat <- as.data.frame(summary(neig_sT))
neig_T_dat <- rownames_to_column(neig_T_dat)
colnames(neig_T_dat) <- c("neighbourhood", "superhosts")

#merge datasets for no. number of listings & no. of superhosts in each neighbourhood
neig_l_s <- merge(neig_dat, neig_T_dat, by = "neighbourhood")
neig_l_s$perc_superhost <- neig_l_s$superhosts / neig_l_s$listings

#rank boroughs according to no. of listings
n_sort_listings <- neig_l_s[order(neig_l_s$listings, decreasing = T),]
#rank boroughs according to percentage of superhost
n_sort_percsup <- neig_l_s[order(neig_l_s$perc_superhost, decreasing = T),]

```

## latitude and longitude
```{r}
lis_location <- listings1[c("latitude", "longitude", "host_is_superhost")]
lis_location$host_is_superhost <- as.factor(lis_location$host_is_superhost)
summary(lis_location)

#load the shapefile of london boroughs
ldn_boroughs <- readOGR(dsn = "London-boroughs/London_Borough_Excluding_MHW.shp") #the whole london-boroughs folder must be downloaded because .shp (shapefile) is composed by different files in the folder.
#transform the coordinates to longitude and latitude
proj4string(ldn_boroughs) <- CRS("+init=epsg:27700")
ldn_boroughs.wgs84 <- spTransform(ldn_boroughs, CRS("+init=epsg:4326"))
#add a column of id for matching with dataframe containing borough name
ldn_boroughs.wgs84$id <- row.names(ldn_boroughs.wgs84)

##(11.1) map one: visualise percentage of superhosts on map of london boroughs
#labels for polygons (boroughs), positioned at the centre of each polygon
centroid.dat <- as.data.frame(coordinates(ldn_boroughs.wgs84))
centroid.dat <- rownames_to_column(centroid.dat, "id")
colnames(centroid.dat) <- c("id", "long", "lat")
polygon_label.dat <- left_join(centroid.dat, ldn_boroughs.wgs84@data, by="id")

#merge shapefile data with borough name data and count of listings / superhosts data
ldn_boroughs.wgs84@data <- left_join(ldn_boroughs.wgs84@data, neig_l_s, by = c("NAME" = "neighbourhood"))
ldn_boroughs_f <- fortify(ldn_boroughs.wgs84)
ldn_boroughs_m <- left_join(ldn_boroughs_f, ldn_boroughs.wgs84@data, by = "id")

map01 <- ggplot() + geom_polygon(data=ldn_boroughs_m, mapping=aes(long, lat, group=`NAME`, fill=`perc_superhost`)) + geom_path(color="white", lwd=0.1) + coord_equal() + scale_fill_gradient(high="#132B43", low="#56B1F7", name = "% of superhosts")
map02 <- map01 + geom_text(data=polygon_label.dat, mapping=aes(x=long, y=lat, label=NAME), size=3) + ggtitle("Percentage of superhosts in each borough")

##(11.2) map two: visualise listings on map of london boroughs
#plot map with listings data
map1 <- ggplot() + geom_polygon(data=ldn_boroughs_m, mapping=aes(long, lat, group = group, fill=NAME)) + geom_path(data=ldn_boroughs_m, mapping=aes(long, lat, group=group), color="white", lwd=0.1)
map2 <- map1 + geom_point(data=lis_location, mapping=aes(longitude, latitude), color="dimgrey", size=0.05) + coord_equal() + ggtitle("Airbnb listings in London")
map3 <- map2 + theme(legend.position = "none") #map without legend (names of borough)
map3

#transform location data to distance to central london
cent_ldn_lat <- 51.509865
cent_ldn_long <- -0.118092

mil_to_centre_n <- sqrt(((lis_location$longitude - cent_ldn_long)*54.6)^2 + ((lis_location$latitude - cent_ldn_lat)*69)^2)
summary(mil_to_centre_n)

listings1$mil_to_centre_n <- mil_to_centre_n
```

[1]  https://toboroughsdatascience.com/plotting-a-map-of-london-crime-data-using-r-8dcefef1c397 the reference for transformation of coordinates to longitude and latitude
[2]  https://www.latlong.net/place/london-the-uk-14153.html centre of london longitude and latitude
[3] https://github.com/valeria-io/crime_london_map (11.2) map 1

- code for maps with uniform colours

map0 <- ggplot() + geom_polygon(data=ldn_boroughs.wgs84, mapping=aes(long, lat, group = group), fill="skyblue2") + geom_path(data=ldn_boroughs.wgs84, mapping=aes(long, lat, group = group), color="white", lwd=0.1)
map0.1 <- map0 + geom_point(data=lis_location, mapping=aes(longitude, latitude), color = "dimgrey", size=0.05) + ggtitle("Airbnb listings in London") + theme(legend.position = "none")


## 3.3 Property Data
## Property Type
```{r}
#check price difference among different types (original)
property_n2 <- listings1$property_type
prop_o_price <- as.data.frame(cbind(property_n2, listings1$price))
prop_o_price$price_n <- as.numeric(prop_o_price$V2)
ggplot(data=prop_o_price, aes(x=factor(property_n2), y=log(price_n)))+geom_boxplot()

#group levels according to common understanding of room types
property_n <- listings1$property_type
summary(as.factor(property_n))
property_n[property_n == "Aparthotel" |  property_n == "Loft" | property_n == "Serviced apartment"] <- "Apartment"
property_n[property_n == "Bungalow" |property_n == "Cottage" | property_n == "Tiny house" | property_n == "Townhouse" ] <- "House"
property_n[property_n == "Villa"] <- "Condominium"
property_n[property_n != "Apartment" & property_n != "House" & property_n != "Condominium"] <- "Others"

#check price difference among different types
prop_price <- as.data.frame(cbind(property_n, listings1$log_price_n))
prop_price$price <- as.numeric(prop_price$V2)
ggplot(data=prop_price, aes(x=factor(property_n), y=price))+geom_boxplot()

mean(prop_price$price[prop_price$property_n == "Apartment"])
mean(prop_price$price[prop_price$property_n == "Condominium"])
mean(prop_)

listings1$property_type_n <- as.factor(property_n)
```

## Room Type

```{r}
summary(as.factor(listings1$room_type))
listings1$room_type <- as.factor(listings1$room_type)
```


## Bed Type

```{r}
summary(as.factor(listings1$bed_type))
#change to real bed and non real bed as other categories have too few data
bed_type_n <- listings1$bed_type
bed_type_n[bed_type_n != "Real Bed"] <- "Non Bed"
summary(as.factor(bed_type_n))
listings1$bed_type_n <- as.factor(bed_type_n)
```

## 3.4 Amenities

```{r}
amenities_n <- listings1$amenities
head(amenities_n)
amenities_list <- strsplit(amenities_n, ",")
head(amenities_list)

#clean amenities list
remove.punc <- function(mylist) {str_remove_all(mylist, "[:punct:]")}
a_clean_list <- lapply(amenities_list, remove.punc)


#plot histogram of no. of amenities for listings
total_amenities <- as.numeric(summary(a_clean_list)[,1])
hist(total_amenities, 
     main = "Histogram of No. of Amenities for Listings",
     xlab = "Number of Amenities",
     ylab = "Number of Listings")

#check relationship between no. of total amenities and host_is_superhost
total_sup <- as.data.frame(cbind(total_amenities, listings1$host_is_superhost))
total_sup$total_amenities <- as.numeric(total_sup$total_amenities)
total_sup$V2 <- as.factor(total_sup$V2)
boxplot(total_sup$total_amenities ~ total_sup$V2,
        main = "Superhost Status VS No. of Amenities",
        xlab = "Is Superhost",
        ylab = "No. of Amenities")
mean(total_sup[(total_sup$V2=="t"),1])
mean(total_sup[(total_sup$V2=="f"),1])


#find all possible items provided
raw_items <- getAllMethods(a_clean_list)

#find the frequency that each item appears
getMethodFreq <- function (LOL, item_list){
  freq_list <- matrix(0, nrow = length(item_list), ncol = 1)
  rownames(freq_list) <- item_list
  colnames(freq_list) <- "freq"
  
  for (m in 1:length(item_list)) {
    count = 0
    for (i in 1:length(LOL)) {
      if (item_list[m] %in% LOL[[i]]) {
        count = count + 1
      }
    freq_list[m][1] = count
    }
  }
  return (freq_list)
}

#get the frequency that each item appears
item_freq <- getMethodFreq(a_clean_list, raw_items)
item_freq <- as.data.frame(item_freq)
item_freq <- rownames_to_column(item_freq, "item")
item_freq <- item_freq[order(item_freq$freq, decreasing = T),]


#count the number of items provided by listings that are among the top N amenities
countTopN <- function (LOL, topN) {
  count_matrix = matrix(NA, nrow = length(LOL), ncol = 1)
    
  for (i in 1:length(LOL)) {
    count = 0
    for (m in 1:length(topN)) {
      if (topN[m] %in% LOL[[i]]) {count = count + 1}
    }
    count_matrix[i][1] = count
  }
  
  return(count_matrix)
}

#(1)check top 20 amenities: count, distribution, relationship to host_is_superhost
top20amen <- item_freq$item[1:20]
top20_count_matrix <- countTopN(a_clean_list, top20amen)
head(top20_count_matrix)

##(1.2)distribution
hist(top20_count_matrix[,1])

##(1.3)rship between no. of items in top20 amenities and host_is_superhost
top20_sup <- as.data.frame(cbind(top20_count_matrix, listings1$host_is_superhost))
colnames(top20_sup) <- c("top20_count", "is_superhost")
top20_sup$top20_count <- as.numeric(top20_sup$top20_count)
top20_sup$is_superhost <- as.factor(top20_sup$is_superhost)
boxplot(top20_sup$top20_count ~ top20_sup$is_superhost) 
t.test(top20_count ~ is_superhost, data = top20_sup) #difference between two levels: significant

#(2)check top 10 amenities:
top10amen <- item_freq$item[1:10]
top10_count_matrix <- countTopN(a_clean_list, top10amen)
head(top10_count_matrix)

##(2.2)distribution
hist(top10_count_matrix[,1])

##(2.3)rship between no. of items in top10 amenities and host_is_superhost
top10_sup <- as.data.frame(cbind(top10_count_matrix, listings1$host_is_superhost))
colnames(top10_sup) <- c("top10_count", "is_superhost")
top10_sup$top10_count <- as.numeric(top10_sup$top10_count)
top10_sup$is_superhost <- as.factor(top10_sup$is_superhost)
boxplot(top10_sup$top10_count ~ top10_sup$is_superhost) #visually: not highly significant
t.test(top10_count ~ is_superhost, data = top10_sup)

#(3)check top 30 amenities:
top30amen <- item_freq$item[1:30]
top30_count_matrix <- countTopN(a_clean_list, top30amen)
head(top30_count_matrix)

##(3.2)distribution
hist(top30_count_matrix[,1])

##(3.3)rship between no. of items in top30 amenities and host_is_superhost
top30_sup <- as.data.frame(cbind(top30_count_matrix, listings1$host_is_superhost))
colnames(top30_sup) <- c("top30_count", "is_superhost")
top30_sup$top30_count <- as.numeric(top30_sup$top30_count)
top30_sup$is_superhost <- as.factor(top30_sup$is_superhost)
boxplot(top30_sup$top30_count ~ top30_sup$is_superhost) #visually: not highly significant
t.test(top30_count ~ is_superhost, data = top30_sup)

#add new column to listings1
listings1$amen_count <- total_amenities
```

conclusion 1: the ten most frequently appeared amenities for listings on Airbnb:
 [1] "Wifi"           "Essentials"     "Heating"        "Kitchen"       
 [5] "Smoke detector" "Washer"         "Hangers"        "Iron"          
 [9] "Hair dryer"     "Shampoo"      

conclusion 2: superhosts tend to provide more amenities than non-superhosts

## 3.5 Popularity
```{r}
#check no. of reviews
summary(listings1$reviews_per_month) #median= 1.16
hist(listings1$reviews_per_month,
     main = "Histogram of Reviews Per Month")
RPM_med = median(listings1$reviews_per_month)
length(which(listings1$reviews_per_month > RPM_med)) 

#check review score
hist(listings1$review_scores_rating, breaks = 50)
summary(listings1$review_scores_rating) #median = 95
RScore_med = median(listings1$review_scores_rating)
length(which(listings1$review_scores_rating > RScore_med))

popular.dat <- as.data.frame(cbind(listings1$reviews_per_month, listings1$review_scores_rating))
popT.index <- which(popular.dat$V1 > RPM_med & popular.dat$V2 > RScore_med)

popular.dat$is_popular <- T
popular.dat$is_popular[-popT.index] <- F

length(popular.dat[popular.dat$is_popular==TRUE,"is_popular"])
length(popular.dat[popular.dat$is_popular==FALSE,"is_popular"])

listings1$is_popular <- popular.dat$is_popular
```

## other variable transformations 
## cancellation_policy

```{r}
cancel_n <- as.factor(listings1$cancellation_policy)
summary(cancel_n)
cancel_n[cancel_n != "flexible" & cancel_n != "moderate"] <- "strict"
cancel_n <- droplevels(cancel_n)
listings1$cancellation_policy_n <- cancel_n
```

Note: within level "strict", 23574 out of 23897 are "strict_14_with_grace_period". The others levels were merged into strict because they have very small presence.

## guest verification

```{r}

summary(listings1$require_guest_profile_picture) #628 t
summary(as.factor(listings1$require_guest_phone_verification)) #999 t

#check rlship between phone_verifcation and profile-pic-verif
length(which(listings1$require_guest_profile_picture == "t" & listings1$require_guest_phone_verification == "t")) #557 t

#transform verification
guest_verif <- as.data.frame(cbind(listings1$require_guest_profile_picture, listings1$require_guest_phone_verification, 0))
colnames(guest_verif) <- c("profil_pic", "phone", "any_form")
for (i in 1:nrow(guest_verif)) {
  if (guest_verif$profil_pic[i] == "t" | guest_verif$phone[i] == "t") {
    guest_verif$any_form[i] <- T
  } else {guest_verif$any_form[i] <- F}
}
summary(as.factor(guest_verif$any_form))

listings1$guest_verif_n <- as.factor(guest_verif$any_form)
```


## 4 Data Analysis with Models
## 4.1 Preparation
```{r}
#subset to include only the relevant ones
colnames(listings1)
relevant_col = c("log_price_n", "host_since_n", "host_response_time_n", "host_response_rate_n", "host_verification_n", "mil_to_centre_n", "property_type_n", "bed_type_n", "amen_count", "cancellation_policy_n", "guest_verif_n", "room_type", "superhost_n", "guests_included","minimum_nights", "reviews_per_month", "review_scores_rating", "is_popular")
listings2 <- listings1[,relevant_col]
summary(listings2)
```

Remove evidently non-relevant columns: id, name, summary, space, description,
experiences_offered, neighborhood_overview, notes, transit, access, interaction,
house_rules, host_id, host_name, host_about.  

Select one variable to represent similar variables:
1. Verification {host verification_n: host_verification, host_has_profile_pic, host_identity_verified}
2. Location {mil_to_centre_n: neighbourhood_cleansed, latitude, longitude}
3. Price {price_n: price, cleaning_fee}
4. No. of Reviews {reviews_per_month: number_of_reviews_ltm, number_of_reviews}
5. Reviews' Ratings {review_scores_rating: review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value}
6. guest verif {guest_verif_n: require_guest_profile_picture,  require_guest_phone_verification}

Some variables appear to have heavy tails (or outliers) in comparing with normal distributions, as they minimum and maximum values are way out of the ranges of mean ±2×SDT. As applying normalisation or standardisation may make interpretation harder and disrupt the natural dependency structure within data, we prefer to leave variables to its original form.


## 2. Model1 - glm popular

```{r}
#create a confusion matrix
ct.op <- function(predicted, observed) {
  df.op <- data.frame(predicted = predicted, observed = observed)
  op.tab <- table(df.op)
  op.tab <- rbind(op.tab, c(round(prop.table(op.tab, 2)[1, 1], 2), round((prop.table(op.tab, 2)[2, 2]), 2)))
  rownames(op.tab) <- c("pred=F", "pred=T", "%corr")
  colnames(op.tab) <- c("obs=F", "obs=T")
  op.tab
}

#remove the two variables that were used to construct is_popular
glm1 <- glm(is_popular ~. - reviews_per_month - review_scores_rating, family=binomial, data=listings2)
summary(glm1)
anova(glm1, test="Chisq")
#set threshold as 0.5
pred.m1 <- as.numeric(glm1$fitted.values > 0.5) 
ct.op(pred.m1, listings2$is_popular) #0.92, 0.39

#remove non-significant variables one by one
#remove host_since_n 
glm2 <- glm(is_popular ~. - reviews_per_month - review_scores_rating - host_since_n, family=binomial, data=listings2)
anova(glm2, test="Chisq") #test the overall significance of a variable
#set threshold as 0.5
pred.m2 <- as.numeric(glm2$fitted.values > 0.5) 
ct.op(pred.m2, listings2$is_popular) #0.92, 0.39

#remove bed_type_n
glm3 <- glm(is_popular ~. - reviews_per_month - review_scores_rating - host_since_n - bed_type_n, family=binomial, data=listings2)
anova(glm3, test="Chisq")
#set threshold as 0.5
pred.m3 <- as.numeric(glm3$fitted.values > 0.5) 
ct.op(pred.m3, listings2$is_popular) #0.92, 0.39

#remove log_price_n
glm4 <- glm(is_popular ~. - reviews_per_month - review_scores_rating - host_since_n - bed_type_n - log_price_n, family=binomial, data=listings2)
anova(glm4, test="Chisq")
#set threshold as 0.5
pred.m4 <- as.numeric(glm4$fitted.values > 0.5) 
ct.op(pred.m4, listings2$is_popular) #0.92, 0.39

#remove mil_to_centre_n
glm5 <- glm(is_popular ~. - reviews_per_month - review_scores_rating - host_since_n - bed_type_n - log_price_n - mil_to_centre_n, family=binomial, data=listings2)
anova(glm5, test="Chisq")
#set threshold as 0.5
pred.m5 <- as.numeric(glm5$fitted.values > 0.5) 
ct.op(pred.m5, listings2$is_popular)

#try interaction between most significant ones
glm6 <- glm(is_popular ~ host_since_n + host_response_time_n + host_response_rate_n + property_type_n + amen_count + cancellation_policy_n + guest_verif_n + room_type + mil_to_centre_n * superhost_n + accommodates, family=binomial, data=listings2)
anova(glm6, test="Chisq")
#set threshold as 0.5
pred.m6 <- as.numeric(glm6$fitted.values > 0.5) 
ct.op(pred.m6, listings2$is_popular) #0.92, 0.45
#significant but does not improve on confusion matrix, disregard glm6

#set threshold
set.seed(101)
g.train.index <- sample(1:dim(listings2)[1], dim(listings2)[1] * 0.8) #80:20 split
g.train.dat <- listings2[g.train.index,]
g.test.dat <- listings2[-g.train.index,]

glm5tr <- glm(is_popular ~ host_response_time_n + host_response_rate_n + property_type_n + amen_count + property_type_n + cancellation_policy_n + guest_verif_n + room_type + superhost_n + guests_included + minimum_nights, data=g.train.dat)
pred.train <- glm5tr$fitted.values

glm5te <- glm(is_popular ~ host_response_time_n + host_response_rate_n + property_type_n + amen_count + property_type_n + cancellation_policy_n + guest_verif_n + room_type + superhost_n + guests_included + minimum_nights, data=g.test.dat)
pred.test <- glm5te$fitted.values

prediction.test <- prediction(pred.test, g.test.dat$is_popular)
prediction.train <- prediction(pred.train, g.train.dat$is_popular)

roc.test <- performance(prediction.test, measure="tpr", x.measure="fpr")
roc.train <- performance(prediction.train, measure="tpr", x.measure="fpr")
par(mfrow=c(1,2))
plot(roc.test, lwd=2, colorkey=T, colorize=T, main="ROC curve on testing data")
abline(0,1)
plot(roc.train, lwd=2, colorkey=T, colorize=T, main="ROC curve on training data")
abline(0,1)

#try glm5, still
pred.m5t <- as.numeric(glm5$fitted.values > 0.45) 
ct.op(pred.m5t, listings2$is_popular) #0.91, 0.47

```

## 4.2 Popularity via Decision Tree

```{r}
#remove two variables used to construct is_popular
listings_dt <- subset(listings2, select=-c(reviews_per_month, review_scores_rating))

set.seed(12)
tree.pop1 <- tree(as.factor(is_popular) ~., data=listings_dt)
summary(tree.pop1) #mis = 0.21
plot(tree.pop1)
text(tree.pop1, pretty=0, cex=0.6)

#assess perfomance - 80:20 split for training and testing
nrow(listings_dt) * 0.8
set.seed(14)
train.index=sample(1:nrow(listings_dt), 31342) #80% of original data used for training
train.dat <- listings_dt[train.index,]
test.dat <- listings_dt[-train.index,]
popular_test <- test.dat$is_popular
tree.pop2 <- tree(as.factor(is_popular) ~., data=train.dat)
summary(tree.pop2) #mis = 0.1882
plot(tree.pop2) 
text(tree.pop2, pretty=0, cex=0.6)
tree.pop2.predict <- predict(tree.pop2, test.dat, type="class")
tree.test.conf <- table(tree.pop2.predict, popular_test) 
(tree.test.conf[2,1] + tree.test.conf[1,2]) / nrow(test.dat) #mis = 0.215
#poor performance on predicting T

#improve decision tree: bagging
dim(listings_dt)
#! Caution: bagging takes more than 7 min to run, check below for results
bag.pop <- randomForest(as.factor(is_popular)~., data=listings_dt, subset=train.index, mtry=15, importance=T) #mtry=15 uses all predictor in the dataset
bag.pop
importance(bag.pop)

#improve decision tree: randomForest
#! Caution: it takes a bit of time to run
rf.pop <- randomForest(as.factor(is_popular)~., data=listings_dt, subset=train.index, mtry=4, importance=T) #mtry sets m=4 close to sqrt(p=15)
rf.pop
importance(rf.pop) #mis=0.181importance(bag.pop)
varImpPlot(rf.pop, col=c("blue", "red"))

#cross validation to determine the tree size
cv.pop <- cv.tree(tree.pop1, FUN=prune.misclass)
cv.pop$size
cv.pop$dev
#pop1 is indeed the cv selected tree
```

! results for bagging: 

Call:
 randomForest(formula = as.factor(is_popular) ~ ., data = listings_dt,      mtry = 15, importance = T, subset = train.index) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 15

        OOB estimate of  error rate: 18.65%
Confusion matrix:
      FALSE TRUE class.error
FALSE 29417 2479  0.07772134
TRUE   5203 4089  0.55994404

! results for random forest

Call:
 randomForest(formula = as.factor(is_popular) ~ ., data = listings_dt,      mtry = 4, importance = T, subset = train.index) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 4

        OOB estimate of  error rate: 18.1%
Confusion matrix:
      FALSE TRUE class.error
FALSE 29637 2259  0.07082393
TRUE   5197 4095  0.55929832

- bagging and rf suggests the decision tree cannot be further improved

```{r}
#use a more balanced dataset to fit deicion tree
listings_dt_T <- listings_dt[which(listings_dt$is_popular == T),]
listings_dt_F <- listings_dt[which(listings_dt$is_popular == F),]
set.seed(16)
listings_dt_bal1 <- getBalanceData(listings_dt_T, listings_dt_F)

tree.bal1 <- tree(is_popular ~., data=listings_dt_bal1)
summary(tree.bal1) #mis = 0.189
plot(tree.bal1)
text(tree.bal1, pretty=0, cex=0.6)

set.seed(18)
train.bal1.index=sample(1:nrow(listings_dt_bal1), nrow(listings_dt_bal1) * 0.8) #80% of original data used for train.bal1
train.bal1.dat <- listings_dt_bal1[train.bal1.index,]
test.bal1.dat <- listings_dt_bal1[-train.bal1.index,]
popular_test.bal1 <- test.bal1.dat$is_popular
tree.bal2 <- tree(as.factor(is_popular) ~., data=train.bal1.dat)
summary(tree.bal2) #mis = 0.2964
plot(tree.bal2) 
text(tree.bal2, pretty=0, cex=0.6)
tree.bal2.predict <- predict(tree.bal2, test.bal1.dat, type="class")
tree.test.bal1.conf <- table(tree.bal2.predict, popular_test.bal1) 
(tree.test.bal1.conf[2,1] + tree.test.bal1.conf[1,2]) / nrow(test.bal1.dat) #mis = 0.285

#the balanced dataset does not bring a better decision tree.
```


## 4.3 Data Analysis with MLR
```{r}
lm.p0 <- lm(review_per_month ~ ., data=listings2)
summary(lm.p0)
vif(lm.p0)

lm.p <- lm(log_price_n ~ mil_to_centre_n + room_type, data=listings2)
summary(lm.p)
```

## 4.4 Data Analysis with Text
```{r}
data(stop_words)


textcolumns = c("name","summary","space","description","host_about")

#loop to repeatedly analyse each column
for (i in 1:(length(textcolumns))){
  # draw text column from dataset
  text1 <- pull(listings1,textcolumns[i])
  text1_df = data_frame(text1)
  text1_tidy = text1_df %>% unnest_tokens(word1, text1)
  # remove stop-words; by required due to different variable names word1 and word
  text1_noS <- anti_join(text1_tidy, stop_words, by=c("word1" = "word"))
  # pipe the words
  text1_noS_piped = text1_noS %>% count(word1, sort=T) %>% filter(n>250)
  assign(paste("topwords_",textcolumns[i],sep=""),text1_noS_piped)
}
```

## Name
```{r}
topwords_name
topwords_name %>% wordcloud2
```

## Summary
```{r}
topwords_summary
topwords_summary %>% wordcloud2
```

## Space
```{r}
topwords_space
topwords_space %>% wordcloud2
```

## Description
```{r}
topwords_description
topwords_description %>% wordcloud2
```

## Host_about
```{r}
topwords_host_about
topwords_host_about %>% wordcloud2
```

## Text Comparison Between Popular and Non-Popular
```{r}
#loop to repeatedly analyse each column, for both popular and non-popular listings
for (i in 1:(length(textcolumns))){
  # draw text column from dataset
  text1 <- listings1[listings1$is_popular==TRUE,textcolumns[i]]
  text1_df = data_frame(text1)
  text1_tidy = text1_df %>% unnest_tokens(word1,textcolumns[i])
  # remove stop-words; by required due to different variable names word1 and word
  text1_noS <- anti_join(text1_tidy, stop_words, by=c("word1" = "word"))
  # pipe the words
  text1_noS_piped = text1_noS %>% count(word1, sort=T) %>% filter(n>250)
  assign(paste("pop_topwords_",textcolumns[i],sep=""),text1_noS_piped)
  
  
  # repeat process for non-popular listings
  text1 <- listings1[listings1$is_popular==FALSE,textcolumns[i]]
  text1_df = data_frame(text1)
  text1_tidy = text1_df %>% unnest_tokens(word1,textcolumns[i])
  # remove stop-words; by required due to different variable names word1 and word
  text1_noS <- anti_join(text1_tidy, stop_words, by=c("word1" = "word"))
  # pipe the words
  text1_noS_piped = text1_noS %>% count(word1, sort=T) %>% filter(n>250)
  assign(paste("Npop_topwords_",textcolumns[i],sep=""),text1_noS_piped)
}

```

## Summary (Popular VS Non-popular)
```{r}
pop_topwords_summary
pop_topwords_summary %>% wordcloud2
Npop_topwords_summary
Npop_topwords_summary %>% wordcloud2
```

## Name (Popular VS Non-popular)
```{r}
pop_topwords_name
pop_topwords_name %>% wordcloud2
Npop_topwords_name
Npop_topwords_name %>% wordcloud2
```

## Space (Popular VS Non-popular)
```{r}
pop_topwords_space
pop_topwords_space %>% wordcloud2
Npop_topwords_space
Npop_topwords_space %>% wordcloud2
```

## Description (Popular VS Non-popular)
```{r}
pop_topwords_description
pop_topwords_description %>% wordcloud2
Npop_topwords_description
Npop_topwords_description %>% wordcloud2
```

## Host_about (Popular VS Non-popular)
```{r}
pop_topwords_host_about
pop_topwords_host_about %>% wordcloud2
Npop_topwords_host_about
Npop_topwords_host_about %>% wordcloud2
```

